{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(profile={'max_input_tokens': 131072, 'max_output_tokens': 8192, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': False, 'tool_calling': True}, client=<groq.resources.chat.completions.Completions object at 0x174012ae0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x16a65b590>, model_name='llama-3.1-8b-instant', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "model=ChatGroq(model=\"llama-3.1-8b-instant\",groq_api_key=groq_api_key)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Nice to meet you, Varsha. As a Chief AI Engineer, I'm sure you're at the forefront of innovation and technology. What brings you here today? Are you working on a new project or would you like to discuss something specific related to AI engineering?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 54, 'prompt_tokens': 49, 'total_tokens': 103, 'completion_time': 0.076076286, 'completion_tokens_details': None, 'prompt_time': 0.003718513, 'prompt_tokens_details': None, 'queue_time': 0.037080501, 'total_time': 0.079794799}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_020e283281', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c92c3-3f57-72c2-88a1-ea08033606bf-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 49, 'output_tokens': 54, 'total_tokens': 103})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage,SystemMessage\n",
    "model.invoke([HumanMessage(content=\"Hi , My Name is Varsha and I am a Chief AI Engineer\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Varsha, and you are a Chief AI Engineer.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 118, 'total_tokens': 133, 'completion_time': 0.022496821, 'completion_tokens_details': None, 'prompt_time': 0.008143787, 'prompt_tokens_details': None, 'queue_time': 0.036611513, 'total_time': 0.030640608}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_d317489708', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c92c3-413a-7643-beca-514d60fb2d8b-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 118, 'output_tokens': 15, 'total_tokens': 133})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import AIMessage\n",
    "model.invoke([\n",
    "    HumanMessage(content=\"Hi , My Name is Varsha and I am a Chief AI Engineer\"),\n",
    "    AIMessage(content=\"Nice to meet you, Varsha. As a Chief AI Engineer, that's a fascinating role. What areas of AI are you currently working on or have expertise in?Are you involved in any exciting projects or innovations in the field? \")\n",
    ",    HumanMessage(content=\"What's my name and what do I do? \")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Message History\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "store={}\n",
    "def get_session_history(session_id:str)->BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id]=ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "with_message_history=RunnableWithMessageHistory(model,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Hello Varsha, it's nice to meet you. Is there something I can help you with or would you like to chat?\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 42, 'total_tokens': 69, 'completion_time': 0.028484711, 'completion_tokens_details': None, 'prompt_time': 0.002224243, 'prompt_tokens_details': None, 'queue_time': 0.036455452, 'total_time': 0.030708954}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_8f8420ecd7', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019c92c3-4871-75f3-bbd3-d54620dd991d-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 42, 'output_tokens': 27, 'total_tokens': 69}\n",
      "content=\"I don't have any information about your name. This conversation has just started, and I don't retain any information about individual users. If you'd like to share your name with me, I can try to remember it for the duration of our conversation.\" additional_kwargs={} response_metadata={'token_usage': {'completion_tokens': 52, 'prompt_tokens': 40, 'total_tokens': 92, 'completion_time': 0.067077292, 'completion_tokens_details': None, 'prompt_time': 0.00252613, 'prompt_tokens_details': None, 'queue_time': 0.036783237, 'total_time': 0.069603422}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_d317489708', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019c92c3-48fd-7123-b301-803eee810e82-0' tool_calls=[] invalid_tool_calls=[] usage_metadata={'input_tokens': 40, 'output_tokens': 52, 'total_tokens': 92}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# Session 1\n",
    "response1 = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi, my name is Varsha\")],\n",
    "    config={\"session_id\": \"chat1\"}\n",
    ")\n",
    "print(response1)\n",
    "\n",
    "# Session 2\n",
    "response2 = with_message_history.invoke(\n",
    "    [HumanMessage(content=\"What's my name?\")],\n",
    "    config={\"session_id\": \"chat2\"}\n",
    ")\n",
    "print(response2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "config={\"configurable\":{\"session_id\":\"chat1\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=with_message_history.invoke(\n",
    "    [HumanMessage(content=\"Hi , My Name is Varsha and I am a Chief AI Engineer\")],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Nice to meet you, Chief AI Engineer Varsha. That's a fascinating role. What specific areas of AI do you specialize in, and what exciting projects have you been working on lately?\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Your name is Varsha.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 144, 'total_tokens': 151, 'completion_time': 0.008662024, 'completion_tokens_details': None, 'prompt_time': 0.010396462, 'prompt_tokens_details': None, 'queue_time': 0.036371794, 'total_time': 0.019058486}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_66c49b4aa6', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c92c3-511d-7393-a87a-cef9719883dd-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 144, 'output_tokens': 7, 'total_tokens': 151})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke([\n",
    "    HumanMessage(content=\"What's my name\")],\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RunnableWithMessageHistory' object has no attribute 'reset_memory'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#change the config\u001b[39;00m\n\u001b[32m      2\u001b[39m config1={\u001b[33m\"\u001b[39m\u001b[33mConfigurable\u001b[39m\u001b[33m\"\u001b[39m:{\u001b[33m\"\u001b[39m\u001b[33msession_id\u001b[39m\u001b[33m\"\u001b[39m:\u001b[33m\"\u001b[39m\u001b[33mchat4\u001b[39m\u001b[33m\"\u001b[39m}}\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mwith_message_history\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset_memory\u001b[49m(session_id=\u001b[33m\"\u001b[39m\u001b[33mchat4\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m response=with_message_history.invoke([\n\u001b[32m      5\u001b[39m     HumanMessage(content=\u001b[33m\"\u001b[39m\u001b[33mWhat\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms my name\u001b[39m\u001b[33m\"\u001b[39m)],\n\u001b[32m      6\u001b[39m     config=config1\n\u001b[32m      7\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Virtual_Environments/Chatbot_env/lib/python3.12/site-packages/pydantic/main.py:1026\u001b[39m, in \u001b[36mBaseModel.__getattr__\u001b[39m\u001b[34m(self, item)\u001b[39m\n\u001b[32m   1023\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[32m   1024\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1025\u001b[39m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'RunnableWithMessageHistory' object has no attribute 'reset_memory'"
     ]
    }
   ],
   "source": [
    "#change the config\n",
    "config1={\"Configurable\":{\"session_id\":\"chat4\"}}\n",
    "with_message_history.reset_memory(session_id=\"chat4\")\n",
    "response=with_message_history.invoke([\n",
    "    HumanMessage(content=\"What's my name\")],\n",
    "    config=config1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Nice to meet you, Chief AI Engineer Varsha. That's a fascinating role. What specific areas of AI do you specialize in, and what exciting projects have you been working on lately?\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat1: Nice to meet you, Varsha. As a Chief AI Engineer, I'm sure you have a fascinating role that involves designing and developing intelligent systems. What specific areas of AI are you currently working on, and what projects are you most passionate about?\n",
      "Chat2: I don't have any information about your name. I'm a large language model, I don't have personal interactions or memories, so I don't retain any information about individual users. Each time you interact with me, it's a new conversation. If you'd like to share your name, I'd be happy to chat with you!\n"
     ]
    }
   ],
   "source": [
    "model1 = ChatGroq(model=\"llama-3.1-8b-instant\", groq_api_key=groq_api_key)\n",
    "chat1 = model1  # or wrap in your chain if needed\n",
    "response1 = chat1.invoke([HumanMessage(content=\"Hi, My Name is Varsha and I am a Chief AI Engineer\")])\n",
    "print(\"Chat1:\", response1.content)\n",
    "\n",
    "# Chat 2 - completely independent\n",
    "model2 = ChatGroq(model=\"llama-3.1-8b-instant\", groq_api_key=groq_api_key)\n",
    "chat2 = model2\n",
    "response2 = chat2.invoke([HumanMessage(content=\"What's my name?\")])\n",
    "print(\"Chat2:\", response2.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Prompt Tempaltes\n",
    "from langchain_core.prompts import ChatPromptTemplate,MessagesPlaceholder\n",
    "prompt=ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"Hey,You are a helpful assistant, Answer all the questions to the best of your ability\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\")\n",
    "\n",
    "])\n",
    "chain=prompt|model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Nice to meet you, Krish. I'm here to help with any questions or topics you'd like to discuss. How's your day going so far?\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 58, 'total_tokens': 90, 'completion_time': 0.038458023, 'completion_tokens_details': None, 'prompt_time': 0.002942411, 'prompt_tokens_details': None, 'queue_time': 0.036848546, 'total_time': 0.041400434}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_8f8420ecd7', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c92c3-5e1b-7142-a1d9-a98e089391a2-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 58, 'output_tokens': 32, 'total_tokens': 90})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"messages\":[HumanMessage(content=\"Hi My name is Krish\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history=RunnableWithMessageHistory(chain,get_session_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello Krish. It's nice to meet you. Is there something I can help you with or would you like to chat?\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config3={\"configurable\":{\"session_id\":\"chat3\"}}\n",
    "response=with_message_history.invoke([\n",
    "    HumanMessage(content=\"Hi My name is Krish\")],\n",
    "    config=config3)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add more complexity\n",
    "prompt1=ChatPromptTemplate.from_messages([\n",
    "    (\"system\",\"Hey,You are a helpful assistant, Answer all the questions to the best of your ability in {language}\"),\n",
    "    MessagesPlaceholder(variable_name=\"messages\")\n",
    "\n",
    "])\n",
    "\n",
    "chain1=prompt1|model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=chain1.invoke({\"messages\":[HumanMessage(content=\"Hi I am Jyothi\")],\"language\": \"telugu\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'హలో జ్యోతి! నేను వినయ్ అనే సహాయకుడిని. మాకు ఏ సమస్యలో సహాయం చేయాలి?'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with_message_history=RunnableWithMessageHistory(chain1,get_session_history,input_messages_key=\"messages\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "config4={\"configurable\":{\"session_id\":\"chat5\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='नमस्ते ज्योति! मैं आपकी सहायता करने के लिए यहाँ हूँ। क्या आपके पास कोई प्रश्न या चिंता है, जिसके लिए मैं आपकी मदद कर सकता हूँ?', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 65, 'prompt_tokens': 62, 'total_tokens': 127, 'completion_time': 0.094472872, 'completion_tokens_details': None, 'prompt_time': 0.003383249, 'prompt_tokens_details': None, 'queue_time': 0.036144547, 'total_time': 0.097856121}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_66c49b4aa6', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c92c3-7184-7223-988d-1265f257ef19-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 62, 'output_tokens': 65, 'total_tokens': 127})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke({'messages':[HumanMessage(content=\"Hi I am Jyothi\")],\"language\":\"Hindi\"},\n",
    "                            config=config4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='आपका नाम ज्योति है।', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 141, 'total_tokens': 153, 'completion_time': 0.016741747, 'completion_tokens_details': None, 'prompt_time': 0.024717643, 'prompt_tokens_details': None, 'queue_time': 0.038616406, 'total_time': 0.04145939}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_66c49b4aa6', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c92c3-7447-7bd0-a9e0-a5f780e3ed62-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 141, 'output_tokens': 12, 'total_tokens': 153})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_message_history.invoke({'messages':[HumanMessage(content=\"What's my name?\")],\"language\":\"Hindi\"},\n",
    "                            config=config4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Managing The conversation history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import SystemMessage,trim_messages\n",
    "\n",
    "\n",
    "\n",
    "trimmer=trim_messages(\n",
    "    max_tokens=70,\n",
    "    strategy=\"last\",\n",
    "    token_counter=lambda msgs: sum(len(m.content.split()) for m in msgs),\n",
    "    include_system=True,\n",
    "    allow_partial=False,\n",
    "    start_on=\"human\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=[\n",
    "    SystemMessage(content=\"You're a good assistant\"),\n",
    "    HumanMessage(content=\"Hi, I am Krish\"),\n",
    "    AIMessage(content=\"Hello Krish! How can I help you today?\"),\n",
    "    HumanMessage(content=\"Can you explain Newton's laws?\"), \n",
    "    AIMessage(content=\"Sure! Newton's laws describe the relationship between the motion of an object and the forces acting on it.\"),\n",
    "    HumanMessage(content=\"How do I make a perfect omelette?\"), \n",
    "    AIMessage(content=\"To make a perfect omelette, whisk eggs thoroughly, use medium heat, and fold gently when cooking.\"),\n",
    "    HumanMessage(content=\"What's the best workout for abs?\"), \n",
    "    AIMessage(content=\"Some of the best ab workouts include planks, bicycle crunches, and leg raises.\"),\n",
    "    HumanMessage(content=\"Which places should I visit in Paris?\"), \n",
    "     AIMessage(content=\"You should definitely see the Eiffel Tower, Louvre Museum, Notre-Dame, and Montmartre.\"),\n",
    "    HumanMessage(content=\"Translate 'Good morning' to French.\"), AIMessage(content=\"'Good morning' in French is 'Bonjour'.\"),\n",
    "    HumanMessage(content=\"Can you recommend some sci-fi movies?\"), AIMessage(content=\"Sure! Try 'Interstellar', 'The Matrix', 'Blade Runner 2049', and 'Inception'.\"),\n",
    "    HumanMessage(content=\"How should I save money for retirement?\"), AIMessage(content=\"Start by creating a budget, contribute to retirement accounts like 401(k) or IRA, and invest wisely.\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content=\"What's the best workout for abs?\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Some of the best ab workouts include planks, bicycle crunches, and leg raises.', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]),\n",
       " HumanMessage(content='Which places should I visit in Paris?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='You should definitely see the Eiffel Tower, Louvre Museum, Notre-Dame, and Montmartre.', additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]),\n",
       " HumanMessage(content=\"Translate 'Good morning' to French.\", additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"'Good morning' in French is 'Bonjour'.\", additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]),\n",
       " HumanMessage(content='Can you recommend some sci-fi movies?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content=\"Sure! Try 'Interstellar', 'The Matrix', 'Blade Runner 2049', and 'Inception'.\", additional_kwargs={}, response_metadata={}, tool_calls=[], invalid_tool_calls=[]),\n",
       " HumanMessage(content='How should I save money for retirement?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trimmer = trim_messages(\n",
    "    max_tokens=80,  # <-- enough to keep sci-fi message\n",
    "    strategy=\"last\",\n",
    "    token_counter=lambda msgs: sum(len(m.content.split()) for m in msgs), # A function to count tokens (len is simple, or use model tokenizer)\n",
    "    start_on=\"human\",\n",
    "    end_on=(\"human\", \"tool\"),\n",
    ")\n",
    "\n",
    "# Your full conversation history\n",
    "messages=[\n",
    "    SystemMessage(content=\"You're a good assistant\"),\n",
    "    HumanMessage(content=\"Hi, I am Krish\"),\n",
    "    AIMessage(content=\"Hello Krish! How can I help you today?\"),\n",
    "    HumanMessage(content=\"Can you explain Newton's laws?\"), \n",
    "    AIMessage(content=\"Sure! Newton's laws describe the relationship between the motion of an object and the forces acting on it.\"),\n",
    "    HumanMessage(content=\"How do I make a perfect omelette?\"), \n",
    "    AIMessage(content=\"To make a perfect omelette, whisk eggs thoroughly, use medium heat, and fold gently when cooking.\"),\n",
    "    HumanMessage(content=\"What's the best workout for abs?\"), \n",
    "    AIMessage(content=\"Some of the best ab workouts include planks, bicycle crunches, and leg raises.\"),\n",
    "    HumanMessage(content=\"Which places should I visit in Paris?\"), \n",
    "     AIMessage(content=\"You should definitely see the Eiffel Tower, Louvre Museum, Notre-Dame, and Montmartre.\"),\n",
    "    HumanMessage(content=\"Translate 'Good morning' to French.\"), AIMessage(content=\"'Good morning' in French is 'Bonjour'.\"),\n",
    "    HumanMessage(content=\"Can you recommend some sci-fi movies?\"), AIMessage(content=\"Sure! Try 'Interstellar', 'The Matrix', 'Blade Runner 2049', and 'Inception'.\"),\n",
    "    HumanMessage(content=\"How should I save money for retirement?\"), AIMessage(content=\"Start by creating a budget, contribute to retirement accounts like 401(k) or IRA, and invest wisely.\"),\n",
    "]\n",
    "\n",
    "\n",
    "# Invoke the trimmer\n",
    "trimmed_messages = trimmer.invoke(messages)\n",
    "trimmed_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You asked me about Sci-Fi movies.'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "chain4=(\n",
    "    RunnablePassthrough.assign(messages=itemgetter(\"messages\")|trimmer)\n",
    "    |prompt|model\n",
    ")\n",
    "response=chain4.invoke(\n",
    "    {\n",
    "    \"messages\":messages+[HumanMessage(content=\"What genre movies i asked you\")],\n",
    "    \"language\":\"English\"\n",
    "\n",
    "    }\n",
    ")\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wrap in the Message History\n",
    "with_message_history=RunnableWithMessageHistory(\n",
    "    chain4,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"messages\",\n",
    ")\n",
    "config5={\"configurable\":{\"session_id\":\"chat7\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=with_message_history.invoke({'messages':[HumanMessage(content=\"What's my name?\")],\"language\":\"English\"},\n",
    "                            config=config5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't have any information about your personal identity, so I don't know your name. This conversation just started, and I'm a large language model, I don't have the ability to retain information about individual users or their names. If you'd like to share your name with me, I'd be happy to chat with you!\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=with_message_history.invoke({'messages':[HumanMessage(content=\"which country places I asked you to visit\")],\"language\":\"English\"},\n",
    "                            config=config5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You haven't asked me to visit any countries yet. This conversation just started, and we're still at the beginning. If you'd like to ask me about visiting a particular country, I'd be happy to help!\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vector Stores and retrievers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatGroq(groq_api_key=groq_api_key,model=\"llama-3.1-8b-instant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "documents = [\n",
    "    Document(\n",
    "        page_content=\"Groq provides ultra-fast inference for LLMs.\",\n",
    "        metadata={\n",
    "            \"source\": \"groq_docs\",\n",
    "            \"page\": 1\n",
    "        }\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"BGE embeddings are optimized for retrieval tasks.\",\n",
    "        metadata={\n",
    "            \"source\": \"embedding_docs\",\n",
    "            \"page\": 2\n",
    "        }\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"LangChain helps build RAG and agent workflows.\",\n",
    "        metadata={\n",
    "            \"source\": \"langchain_docs\",\n",
    "            \"page\": 3\n",
    "        }\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##What is document\n",
    "from langchain_community.embeddings import HuggingFaceBgeEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/varshareddyjwala/Documents/Virtual_Environments/Chatbot_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading weights: 100%|██████████| 103/103 [00:00<00:00, 1803.63it/s, Materializing param=pooler.dense.weight]                             \n",
      "\u001b[1mBertModel LOAD REPORT\u001b[0m from: sentence-transformers/all-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    }
   ],
   "source": [
    "embeddings=HuggingFaceBgeEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "vectordb=Chroma.from_documents(documents,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "response=vectordb.similarity_search(\"Groq\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='c84a02e6-aa2b-4b13-b0c8-666e0f7beaff', metadata={'page': 1, 'source': 'groq_docs'}, page_content='Groq provides ultra-fast inference for LLMs.'),\n",
       " Document(id='835bac1d-b709-47e6-9408-f5fe8846184e', metadata={'page': 2, 'source': 'embedding_docs'}, page_content='BGE embeddings are optimized for retrieval tasks.'),\n",
       " Document(id='6a5d32f2-5098-4b03-ba6e-7c778ebde14d', metadata={'source': 'langchain_docs', 'page': 3}, page_content='LangChain helps build RAG and agent workflows.')]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VectorDB as Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(id='c84a02e6-aa2b-4b13-b0c8-666e0f7beaff', metadata={'page': 1, 'source': 'groq_docs'}, page_content='Groq provides ultra-fast inference for LLMs.')],\n",
       " [Document(id='835bac1d-b709-47e6-9408-f5fe8846184e', metadata={'page': 2, 'source': 'embedding_docs'}, page_content='BGE embeddings are optimized for retrieval tasks.')]]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "retriever=RunnableLambda(vectordb.similarity_search).bind(k=1)\n",
    "retriever.batch([\"groq\",\"bge\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(id='c84a02e6-aa2b-4b13-b0c8-666e0f7beaff', metadata={'source': 'groq_docs', 'page': 1}, page_content='Groq provides ultra-fast inference for LLMs.')],\n",
       " [Document(id='835bac1d-b709-47e6-9408-f5fe8846184e', metadata={'page': 2, 'source': 'embedding_docs'}, page_content='BGE embeddings are optimized for retrieval tasks.')]]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever1=vectordb.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\":1}\n",
    ")\n",
    "retriever1.batch([\"groq\",\"bge\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "##RAG\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "message=\"\"\"\"\n",
    "Answer this question using the provided context only\n",
    "\n",
    "{question}\n",
    "\n",
    "content:\n",
    "{content}\n",
    "\"\"\"\n",
    "prompt=ChatPromptTemplate.from_messages([(\"human\",message)])\n",
    "rag_chain={\"content\":retriever1,\"question\":RunnablePassthrough()}|prompt|llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Groq provides ultra-fast inference for Large Language Models (LLMs).', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 114, 'total_tokens': 129, 'completion_time': 0.016668166, 'completion_tokens_details': None, 'prompt_time': 0.007031247, 'prompt_tokens_details': None, 'queue_time': 0.056008901, 'total_time': 0.023699413}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_020e283281', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c92da-aac5-7e53-ac70-c3670d0c6d58-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 114, 'output_tokens': 15, 'total_tokens': 129})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response=rag_chain.invoke(\"Tell me about groq\")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "response2=rag_chain.invoke(\"Tell me more about it?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Groq provides ultra-fast inference for Large Language Models (LLMs).', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 114, 'total_tokens': 129, 'completion_time': 0.016668166, 'completion_tokens_details': None, 'prompt_time': 0.007031247, 'prompt_tokens_details': None, 'queue_time': 0.056008901, 'total_time': 0.023699413}, 'model_name': 'llama-3.1-8b-instant', 'system_fingerprint': 'fp_020e283281', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None, 'model_provider': 'groq'}, id='lc_run--019c92da-aac5-7e53-ac70-c3670d0c6d58-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 114, 'output_tokens': 15, 'total_tokens': 129})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Chatbot_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
